{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#               Jigsaw Unintended Bias in Toxicity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Business Problem\n",
    "\n",
    "<h3> 1.1 Description:</h3>\n",
    "    The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation.\n",
    "    A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\n",
    "    They use data by human raters to improve civility in online conversations for various toxic conversational attributes.\n",
    "    \n",
    "   <h3>1.2 Context:</h3>\n",
    "    This is a Kaggle competetion: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview\n",
    "    \n",
    "   <h3>1.3 Data:</h3>\n",
    "    train.csv, test.csv (Download from Kaggle)\n",
    "    \n",
    "   <h3>1.4 Output to be submitted:</h3>\n",
    "    It is in format submission.csv (Download from Kaggle)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Machine Learning Problem Foundation\n",
    "\n",
    "<h2>2.1 Data overview</h2>\n",
    "\n",
    "Attribute information:\n",
    "* comment_text: text of individual comments  \n",
    "\n",
    "* target: toxicity label( to be predicted to for test data. target>=0.5 will be consider to be postive class(toxic))  \n",
    "\n",
    "\n",
    "Identity Attributes:\n",
    "* male\n",
    "* female\n",
    "* transgender\n",
    "* other_gender\n",
    "* heterosexual\n",
    "* homosexual_gay_or_lesbian\n",
    "* bisexual\n",
    "* other_sexual_orientation\n",
    "* christian\n",
    "* jewish\n",
    "* muslim\n",
    "* hindu\n",
    "* buddhist\n",
    "* atheist\n",
    "* other_religion\n",
    "* black\n",
    "* white\n",
    "* asian\n",
    "* latino\n",
    "* other_race_or_ethnicity\n",
    "* physical_disability\n",
    "* intellectual_or_learning_disability\n",
    "* psychiatric_or_mental_illness\n",
    "* other_disability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data=pd.read_csv(\"../Data/train.csv\")\n",
    "test_data=pd.read_csv(\"../Data/test.csv\")\n",
    "#submission = pd.read_csv('../Data/sample_submission.csv', index_col='id')\n",
    "\n",
    "#tr_data=pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "#test_data=pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "#submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tr_data (1804874, 45)\n",
      "Shape of test_data (97320, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of tr_data\",tr_data.shape)\n",
    "print(\"Shape of test_data\",test_data.shape)\n",
    "\n",
    "tr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data overview\n",
    "Attribute information:\n",
    "* comment_text: text of individual comments\n",
    "* target: toxicity label( to be predicted to for test data. target>=0.5 will be consider to be positive class(toxic))\n",
    "\n",
    "\n",
    "When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways.Â \n",
    "There are some identity attributes which are taken care:\n",
    "asian, atheist, bisexual, black, buddhist, christian, female, heterosexual, hindu, homosexual_gay_or_lesbian, intellectual_or_learning_disability, jewish, latino, male, muslim, other_disability, other_gender, other_race_or_ethnicity, other_religion, other_sexual_orientation, physical_disability, psychiatric_or_mental_illness, transgender, white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>...</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text    target  male  female  \\\n",
       "0  This is so cool. It's like, 'would you want yo...  0.000000   NaN     NaN   \n",
       "1  Thank you!! This would make my life a lot less...  0.000000   NaN     NaN   \n",
       "2  This is such an urgent design problem; kudos t...  0.000000   NaN     NaN   \n",
       "3  Is this something I'll be able to install on m...  0.000000   NaN     NaN   \n",
       "4               haha you guys are a bunch of losers.  0.893617   0.0     0.0   \n",
       "\n",
       "   transgender  other_gender  heterosexual  homosexual_gay_or_lesbian  \\\n",
       "0          NaN           NaN           NaN                        NaN   \n",
       "1          NaN           NaN           NaN                        NaN   \n",
       "2          NaN           NaN           NaN                        NaN   \n",
       "3          NaN           NaN           NaN                        NaN   \n",
       "4          0.0           0.0           0.0                        0.0   \n",
       "\n",
       "   bisexual  other_sexual_orientation  ...  other_religion  black  white  \\\n",
       "0       NaN                       NaN  ...             NaN    NaN    NaN   \n",
       "1       NaN                       NaN  ...             NaN    NaN    NaN   \n",
       "2       NaN                       NaN  ...             NaN    NaN    NaN   \n",
       "3       NaN                       NaN  ...             NaN    NaN    NaN   \n",
       "4       0.0                       0.0  ...             0.0    0.0    0.0   \n",
       "\n",
       "   asian  latino  other_race_or_ethnicity  physical_disability  \\\n",
       "0    NaN     NaN                      NaN                  NaN   \n",
       "1    NaN     NaN                      NaN                  NaN   \n",
       "2    NaN     NaN                      NaN                  NaN   \n",
       "3    NaN     NaN                      NaN                  NaN   \n",
       "4    0.0     0.0                      0.0                  0.0   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                 0.25                            0.0   \n",
       "\n",
       "   other_disability  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset with comment text, target and identity_attributes and drop other columns from train_data\n",
    "identity_attribute=['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian','bisexual','other_sexual_orientation',\n",
    "                    'christian','jewish','muslim','hindu','buddhist','atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n",
    "                    'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness','other_disability']\n",
    "\n",
    "data=tr_data[['comment_text','target']]\n",
    "data1=tr_data[identity_attribute]\n",
    "data=pd.concat([data,data1],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_attribute:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "data = convert_dataframe_to_bool(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>...</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  target   male  female  \\\n",
       "0  This is so cool. It's like, 'would you want yo...   False  False   False   \n",
       "1  Thank you!! This would make my life a lot less...   False  False   False   \n",
       "2  This is such an urgent design problem; kudos t...   False  False   False   \n",
       "3  Is this something I'll be able to install on m...   False  False   False   \n",
       "4               haha you guys are a bunch of losers.    True  False   False   \n",
       "\n",
       "   transgender  other_gender  heterosexual  homosexual_gay_or_lesbian  \\\n",
       "0        False         False         False                      False   \n",
       "1        False         False         False                      False   \n",
       "2        False         False         False                      False   \n",
       "3        False         False         False                      False   \n",
       "4        False         False         False                      False   \n",
       "\n",
       "   bisexual  other_sexual_orientation  ...  other_religion  black  white  \\\n",
       "0     False                     False  ...           False  False  False   \n",
       "1     False                     False  ...           False  False  False   \n",
       "2     False                     False  ...           False  False  False   \n",
       "3     False                     False  ...           False  False  False   \n",
       "4     False                     False  ...           False  False  False   \n",
       "\n",
       "   asian  latino  other_race_or_ethnicity  physical_disability  \\\n",
       "0  False   False                    False                False   \n",
       "1  False   False                    False                False   \n",
       "2  False   False                    False                False   \n",
       "3  False   False                    False                False   \n",
       "4  False   False                    False                False   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness  \\\n",
       "0                                False                          False   \n",
       "1                                False                          False   \n",
       "2                                False                          False   \n",
       "3                                False                          False   \n",
       "4                                False                          False   \n",
       "\n",
       "   other_disability  \n",
       "0             False  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing\n",
    "\n",
    "In the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Expand contractions\n",
    "8. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing steps to delete and isolate unwanted symbols\n",
    "symbols_to_delete = 'â†’â˜…Â©Â®â—Ëâ˜†Â¶ï¼‰Ð¸Ê¿ã€‚ï¬‚ï¬â‚â™­å¹´â–ªâ†Ê’ã€ï¼ˆæœˆâ– â‡ŒÉ¹Ë¤Â³ã®Â¤â€¿Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰\\nï¼¼ðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014â‰ \\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶â¤ï¸â˜º\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø£ðŸ˜ðŸ’–Ì¶ðŸ’µâ¥â”â”£â”«Ð•â”—ï¼¯â–ºðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´Êá´‡á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢âœ”\\x96\\x92ðŸ˜‹ðŸ‘ðŸ˜±â€¼\\x81ã‚¸æ•…éšœâž¤\\u2009ðŸšŒÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜â˜•â™¡â—â•‘â–¬ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œÛ©ÛžðŸ˜–á´ ðŸš²âœ’âž¥ðŸ˜ŸðŸ˜ˆâ•ËŒðŸ’ªðŸ™ðŸŽ¯â—„ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®á½²á¼€Î¯á¿ƒá¼´ðŸ™„âœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼¨ï¼©ï¼´ðŸ˜ \\ufeffâ˜»\\u2028ðŸ˜‰ðŸ˜¤â›ºâ™ðŸ™‚\\u3000ðŸ‘®ðŸ’™ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†âœ“â—¾ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08ØŸðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨â¬…ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šç„â„…Ð’ÐŸÐÐðŸ¾ðŸ•â£ðŸ˜†ðŸ”—ðŸš½èˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸â™«Ñ•ï¼£ï¼­â¤µðŸ†ðŸŽƒðŸ˜©â–ˆâ–“â–’â–‘\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’Ž\\x95ðŸ–ðŸ™…â›²ðŸ°â­ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004â§â–°â–”á´¼á´·â—žâ–€\\x13ðŸš¬â–‚â–ƒâ–„â–…â–†â–‡â†™ðŸ¤“\\ue602ðŸ˜µÎ¬ÏŒÎ­á½¸Ì„ðŸ˜’Íâ˜¹âž¡ðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7âœ‹\\uf04c\\x9f\\x10ðŸ˜£âºÌ²Ì…ðŸ˜ŒðŸ¤‘ÌðŸŒðŸ˜¯ðŸ˜²âˆ™â€›á¼¸á¾¶á½ðŸ’žðŸš“â—‡ðŸ””ðŸ“šâœðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613è±†ðŸ¡â–·â”â“â‰â—\\u202fðŸ‘ à¥ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡ðŸŒžËšðŸŽ²ðŸ˜›Ë™å…³ç³»Ð¡ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽâœ¨æ˜¯\\x80\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼â—•à¼½ðŸ˜°á¸·Ð—â–±ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®é™ï¼…ä½ å¤±åŽ»æ‰€é’±æ‹¿åç¨Žéª—ðŸÂ¯ðŸŽ…\\x85ðŸºØ¢Ø¥ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´âŒ Ð˜ÐžÐ Ð¤Ð”Ð¯Ðœâœ˜ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œç¾¤â•ªðŸ’¨åœ†å›­â–¶â„â˜­âœ­ðŸˆðŸ˜ºâ™ªðŸŒâá»‡ðŸ”ðŸ®ðŸâ˜”ðŸ†ðŸ‘ðŸŒ®ðŸŒ¯â˜ ðŸ¤¦\\u200dâ™‚ð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–ÐšðŸ€ðŸ˜«ðŸ¤¤á¿¦åœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºâ˜ƒðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜â˜ŽðŸ¤ ðŸ‘©âœˆðŸ–’âœŒâœ°â†â˜™ðŸšªâš²\\u2006âš­âš†â¬­â¬¯â–â—‹â€£âš“âˆŽâ„’â–™â˜â…›âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹ð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºï½ƒÏ–\\u2000Ò¯ï½á´¦áŽ¥Ò»Íº\\u2007ï½“Ç€\\u2001É©â„®ï½™ï½…àµ¦ï½ŒÆ½Â¸ï½—ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨á‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹âˆ¼ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹â„³ð€ð¥ðªâ„ðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸â˜¼íŒ¨í‹°ï¼·â‹†ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Í¡à¹Ì¯ï´¿âš¾âš½Î¦â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›ðŸŽ¾ðŸ‘¹ï¿¦âŽŒðŸ’â›¸å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ð’‘ð’šð’ð‘´ðŸ¤™ðŸ’â„ƒæ¬¢è¿Žæ¥åˆ°æ‹‰æ–¯ð™«â©â˜®ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»âš ðŸ¦„å·¨æ”¶èµ¢å¾—é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—âœŠðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·âŒâ­•â–¸ð—¢ðŸ³ðŸ±ðŸ¬â¦æ ªå¼â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šâ˜â˜‘å¤šä¼¦âš¡â˜„Ç«ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹â•­â•®ðŸ·ðŸ¦†ä¸ºå‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœä¼ æ•™æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›é¢˜æ—¶å€™ä¾‹æˆ˜èƒœå› åœ£æŠŠå…¨ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ï¼žÊ•Ì£Î”ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©âœžðŸ”«ðŸ‘â”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ˜’å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿â˜ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘âœ…â˜›ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨â™©ðŸŽðŸ¤žâ˜žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ðŸ­ð‘¥ð‘¦ð‘§ï¼¡ï¼®ï¼§ï¼ªï¼¢ðŸ‘£\\uf020â—”â—¡ðŸ‰ðŸ’­ðŸŽ¥â™€ÎžðŸ´ðŸ‘¨ðŸ¤³â¬†ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ì±â„ð‘®ð—•ð—´\\x91ðŸ’â €êœ¥â²£â²â•šðŸ‘â°â†ºâ‡¤âˆé‰„ä»¶âœ¾â—¦â™¬Ñ—ðŸ’Š\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½è™šå½å±ç†å±ˆï½œÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤âˆµâˆ´ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡á¿–Î›Î©â¤ðŸ‡³ð’™ÕÕ¼Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«â˜œÎ’ÏŽðŸ’¢â–²ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´â†³ðŸ’’âŠ˜â–«È»â¬‡ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽâœ§ðŸ˜¼ðŸ•·ï½‡ï½ï½–ï½’ï½Žï½ï½”ï½‰ï½„ï½•ï¼’ï¼ï¼˜ï½†ï½‚ï¼‡ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦âˆ•ðŸŒˆðŸ”­ðŸŠðŸ\\uf10aË†âšœâ˜Ú¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œðŸ”¼'\n",
    "symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±É¡|Â¢`â€•ÉªÂ£â™¥Â´Â¹â‰ˆÃ·â€²É”â‚¬â€ Î¼Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½ÊƒÂ±ÂµÂºÂ¾ï¼ŽÂ»Ð°Ð²â‹…Â¿Â¬Î²â‡’â€ºÂ¡â‚‚â‚ƒÎ³â€³Â«Ï†â…“â€žï¼šÂ¥ÑÉ‘ï¼âˆ’Â²ÊŒÂ¼â´â„â‚„â€šâ€–âŠ‚â…”Â¨Ã—Î¸ï¼Ÿâˆ©ï¼ŒÉâ‚€â‰¥â†‘â†“ï¼âˆšï¼â€°â‰¤'\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contraction map to decontract the phrases\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code ref: https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"decontract the phrases\"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English stopwards to be removed\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mukesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Implementing all pre-processing steps\n",
    "from tqdm import tqdm\n",
    "preprocessed_data = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "# tqdm is for printing the status bar\n",
    "for sentance in (data['comment_text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)  #Removing http/https tag\n",
    "    sentance = expand_contractions(sentance) # expanded short words like n't to not\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text() # Removing xml tags\n",
    "    sentance = contractions.fix(sentance)\n",
    "    sentance = sentance.translate(remove_dict)\n",
    "    sentance = sentance.translate(isolate_dict)\n",
    "    \n",
    "\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip() # Remove words with numbers\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance) # remove special characters\n",
    "    sentance = stemmer.stem(sentance)\n",
    "    sentance = lemmatizer.lemmatize(sentance)\n",
    "    preprocessed_data.append(' '.join(token.lower() for token in nltk.word_tokenize(sentance) if token.lower() not in stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\n",
      "--------------------------------------------------\n",
      "cool like would want mother read really great idea well done\n",
      "==================================================\n",
      "Loving this collection. Cant wait till Season 2 is released. Should be any day now according to http://yeezy-season2.com/\n",
      "--------------------------------------------------\n",
      "loving collection not wait till season released day according\n",
      "==================================================\n",
      "The preserve was sold big time to the public, the only version available to the public before the ballot was the version with the preserve. Yes it was deleted from the ballot language, but that is a pull the wool over the eyes trick that gives Metro the excuse to not be accountable. But they and the zoo are accountable for their statements -- which were unequivacally that there would be an offiste preserve for the elephants.  This just proves that we cannot trust our government to tell the truth. But we still need to hold them accountable for their statements to the public.  And since there was so much public outcry about the preserve, Metro did option land, did hold hearings to discuss the plans for the preserve and did target in 2008 $12 million for captital expenses and a feasibility study. So why claim it was never part of the bond? Because all the money was spent on the new exhibit? Or it was a ruse to fool the public about their real intentions to never provide a preserve.\n",
      "--------------------------------------------------\n",
      "preserve sold big time public version available public ballot version preserve yes deleted ballot language pull wool eyes trick gives metro excuse not accountable zoo accountable statements unequivacally would offiste preserve elephants proves not trust government tell truth still need hold accountable statements public since much public outcry preserve metro option land hold hearings discuss plans preserve target million captital expenses feasibility study claim never part bond money spent new exhibit ruse fool public real intentions never provide preserve\n",
      "==================================================\n",
      "Comfort and your respect for it aside, using epidurals and medications during labor and birth introduces risks also. Setting up any option as one with \"absolutely zero risk\" is fallacy. This is the excellent thing about choice in our community, we get to evaluate and choose our risks relative to our personal values. (This is where one would ensure \"informed choice\".)\n",
      "\n",
      "There are also plenty of mammals who do birth in water, and this article didn't spend much time on people laboring in water and then \"hauling out\" to actually deliver. That happens quite a bit--I'd be interested in seeing stats but anecdotally, I think it's 50/50 that labor happens with water support but birth happens out of water.\n",
      "\n",
      "The lead of the article sums it up, factual research about the safety of water birth is drawing praise and worry for the option. With better information comes better training and technology so expecting parents and their care providers can work together toward safer outcomes.\n",
      "--------------------------------------------------\n",
      "comfort respect aside using epidurals medications labor birth introduces risks also setting option one absolutely zero risk fallacy excellent thing choice community get evaluate choose risks relative personal values one would ensure informed choice also plenty mammals birth water article not spend much time people laboring water hauling actually deliver happens quite bit would interested seeing stats anecdotally think labor happens water support birth happens water lead article sums factual research safety water birth drawing praise worry option better information comes better training technology expecting parents care providers work together toward safer outcomes\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print some sample values of text before and after pre-processing\n",
    "\n",
    "print(data['comment_text'].values[0])\n",
    "print('-'*50)\n",
    "print(preprocessed_data[0])\n",
    "print('='*50)\n",
    "\n",
    "print(data['comment_text'].values[100])\n",
    "print('-'*50)\n",
    "print(preprocessed_data[100])\n",
    "print('='*50)\n",
    "\n",
    "print(data['comment_text'].values[456])\n",
    "print('-'*50)\n",
    "print(preprocessed_data[456])\n",
    "print('='*50)\n",
    "\n",
    "print(data['comment_text'].values[10000])\n",
    "print('-'*50)\n",
    "print(preprocessed_data[10000])\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>christian</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>preprocessed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cool like would want mother read really great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>thank would make life lot less anxiety inducin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>urgent design problem kudos taking impressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>something able install site releasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>haha guys bunch losers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   male  female  transgender  other_gender  heterosexual  \\\n",
       "0   False  False   False        False         False         False   \n",
       "1   False  False   False        False         False         False   \n",
       "2   False  False   False        False         False         False   \n",
       "3   False  False   False        False         False         False   \n",
       "4    True  False   False        False         False         False   \n",
       "\n",
       "   homosexual_gay_or_lesbian  bisexual  other_sexual_orientation  christian  \\\n",
       "0                      False     False                     False      False   \n",
       "1                      False     False                     False      False   \n",
       "2                      False     False                     False      False   \n",
       "3                      False     False                     False      False   \n",
       "4                      False     False                     False      False   \n",
       "\n",
       "   ...  black  white  asian  latino  other_race_or_ethnicity  \\\n",
       "0  ...  False  False  False   False                    False   \n",
       "1  ...  False  False  False   False                    False   \n",
       "2  ...  False  False  False   False                    False   \n",
       "3  ...  False  False  False   False                    False   \n",
       "4  ...  False  False  False   False                    False   \n",
       "\n",
       "   physical_disability  intellectual_or_learning_disability  \\\n",
       "0                False                                False   \n",
       "1                False                                False   \n",
       "2                False                                False   \n",
       "3                False                                False   \n",
       "4                False                                False   \n",
       "\n",
       "   psychiatric_or_mental_illness  other_disability  \\\n",
       "0                          False             False   \n",
       "1                          False             False   \n",
       "2                          False             False   \n",
       "3                          False             False   \n",
       "4                          False             False   \n",
       "\n",
       "                                   preprocessed_data  \n",
       "0  cool like would want mother read really great ...  \n",
       "1  thank would make life lot less anxiety inducin...  \n",
       "2      urgent design problem kudos taking impressive  \n",
       "3              something able install site releasing  \n",
       "4                             haha guys bunch losers  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding preprocessed_data to dataframe and drop comment_text column\n",
    "final=data\n",
    "final['preprocessed_data']=preprocessed_data\n",
    "final.head()\n",
    "final=final.drop(\"comment_text\",axis=1)\n",
    "final.head()\n",
    "#final_original=final\n",
    "#final = final.sample(n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1209265, 26) (1209265,) (595609, 26) (595609,)\n"
     ]
    }
   ],
   "source": [
    "#Data Splitting in to train and cv\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(final,final['target'], test_size=0.33, shuffle=True) \n",
    "\n",
    "print(X_train.shape,y_train.shape,X_cv.shape,y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>christian</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>preprocessed_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457810</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>enjoyed column also enjoyed news today kodak b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682152</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>not disagree post general not police dept exis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463850</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>actually spoke jewish irony pointing peter par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124330</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ok deal taking early rrsp rrif income yes draw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864802</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>could not agree men lost ability play level pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target   male  female  transgender  other_gender  heterosexual  \\\n",
       "457810    False  False   False        False         False         False   \n",
       "1682152   False  False   False        False         False         False   \n",
       "463850    False  False   False        False         False         False   \n",
       "1124330   False  False   False        False         False         False   \n",
       "864802    False   True    True        False         False         False   \n",
       "\n",
       "         homosexual_gay_or_lesbian  bisexual  other_sexual_orientation  \\\n",
       "457810                       False     False                     False   \n",
       "1682152                      False     False                     False   \n",
       "463850                       False     False                     False   \n",
       "1124330                      False     False                     False   \n",
       "864802                       False     False                     False   \n",
       "\n",
       "         christian  ...  black  white  asian  latino  other_race_or_ethnicity  \\\n",
       "457810       False  ...  False  False  False   False                    False   \n",
       "1682152      False  ...  False  False  False   False                    False   \n",
       "463850       False  ...  False  False  False   False                    False   \n",
       "1124330      False  ...  False  False  False   False                    False   \n",
       "864802       False  ...  False  False  False   False                    False   \n",
       "\n",
       "         physical_disability  intellectual_or_learning_disability  \\\n",
       "457810                 False                                False   \n",
       "1682152                False                                False   \n",
       "463850                 False                                False   \n",
       "1124330                False                                False   \n",
       "864802                 False                                False   \n",
       "\n",
       "         psychiatric_or_mental_illness  other_disability  \\\n",
       "457810                           False             False   \n",
       "1682152                          False             False   \n",
       "463850                           False             False   \n",
       "1124330                          False             False   \n",
       "864802                           False             False   \n",
       "\n",
       "                                         preprocessed_data  \n",
       "457810   enjoyed column also enjoyed news today kodak b...  \n",
       "1682152  not disagree post general not police dept exis...  \n",
       "463850   actually spoke jewish irony pointing peter par...  \n",
       "1124330  ok deal taking early rrsp rrif income yes draw...  \n",
       "864802   could not agree men lost ability play level pl...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 5. Featurization : word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to Train your own Word2Vec model using your own text corpus\n",
    "\n",
    "list_of_sentance_train=[]\n",
    "for sentance in X_train['preprocessed_data']:\n",
    "    list_of_sentance_train.append(sentance.split())\n",
    "\n",
    "\n",
    "list_of_sentance_cv=[]\n",
    "for sentance in X_cv['preprocessed_data']:\n",
    "    list_of_sentance_cv.append(sentance.split())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'disagree', 'post', 'general', 'not', 'police', 'dept', 'existing', 'building']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_sentance_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209265"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_sentance_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 319910 unique tokens.\n",
      "(1209265, 220)\n",
      "(1209265, 2)\n",
      "(595609, 220)\n",
      "(595609, 2)\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# Vectorize the text \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#MAX_NUM_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 220\n",
    "#tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list_of_sentance_train + list_of_sentance_cv)\n",
    "sequences = tokenizer.texts_to_sequences(list_of_sentance_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) # fixed the length of words in a row and use padding for that\n",
    "train_labels = to_categorical(np.asarray(y_train))\n",
    "\n",
    "cv_data=pad_sequences(tokenizer.texts_to_sequences(list_of_sentance_cv), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "cv_labels=to_categorical(np.asarray(y_cv))\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(cv_data.shape)\n",
    "print(cv_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "# use glove.840B.300d.txt for word embeddings\n",
    "#GloVe is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "#Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "embeddings_index = {}\n",
    "#EMBEDDING_FILE=\"../input/glove840b300dtxt/glove.840B.300d.txt\"\n",
    "EMBEDDING_FILE='../data/glove.840B.300d.txt'\n",
    "EMBEDDINGS_DIMENSION=300\n",
    "f = open(EMBEDDING_FILE,encoding=\"utf-8\") \n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.036775 ,  0.40917  , -0.52141  , -0.067184 ,  0.087702 ,\n",
       "       -0.048564 ,  0.40947  , -0.42818  ,  0.19304  ,  2.3925   ,\n",
       "       -0.11441  , -0.22952  , -0.16061  ,  0.035533 , -0.53179  ,\n",
       "        0.19764  , -0.48827  ,  0.57439  , -0.064301 ,  0.47053  ,\n",
       "       -0.29647  , -0.15927  , -0.052798 ,  0.10121  , -0.054461 ,\n",
       "        0.036129 , -0.16118  , -0.34139  ,  0.45834  , -0.20144  ,\n",
       "       -0.29067  , -0.51888  , -0.062106 ,  0.14084  ,  0.016413 ,\n",
       "        0.050826 ,  0.13243  , -0.033663 , -0.42228  , -0.30086  ,\n",
       "        0.06202  ,  0.26338  ,  0.077223 ,  0.27307  ,  0.13392  ,\n",
       "        0.30183  , -0.16546  ,  0.057011 , -0.0034585, -0.071113 ,\n",
       "       -0.27287  , -0.10297  ,  0.07457  , -0.32104  ,  0.36696  ,\n",
       "        0.27051  , -0.15776  ,  0.2978   , -0.18988  ,  0.097477 ,\n",
       "        0.035665 , -0.49749  , -0.52759  , -0.046148 ,  0.021715 ,\n",
       "       -0.11047  , -0.18007  ,  0.20295  ,  0.15254  , -0.045976 ,\n",
       "       -0.21846  , -0.066865 , -0.21355  ,  0.017509 ,  0.66474  ,\n",
       "        0.25527  ,  0.24864  , -0.094851 , -0.012857 ,  0.46896  ,\n",
       "        0.052031 ,  0.62488  , -0.12662  ,  0.063972 , -0.15719  ,\n",
       "       -0.45907  ,  0.32286  , -0.17502  ,  0.64181  ,  0.091587 ,\n",
       "       -0.075871 ,  0.11718  , -0.13864  ,  0.24951  , -0.40664  ,\n",
       "        0.08845  , -0.29196  , -0.51624  , -0.074847 , -0.012822 ,\n",
       "       -0.088844 , -0.19935  ,  0.052734 , -0.13588  ,  0.231    ,\n",
       "       -0.34368  ,  0.30607  , -0.21223  ,  0.08178  ,  0.10097  ,\n",
       "        0.33585  , -0.17491  ,  0.019115 ,  0.15998  ,  0.38803  ,\n",
       "       -0.35932  ,  0.31682  , -0.18614  ,  0.11732  , -0.068517 ,\n",
       "        0.50785  , -0.0035486,  0.20069  ,  0.25218  ,  0.38309  ,\n",
       "        0.19359  ,  0.43857  , -0.29954  , -0.14219  ,  0.087962 ,\n",
       "       -0.14229  ,  0.10075  , -0.58986  , -0.12672  ,  0.036944 ,\n",
       "       -0.050421 , -0.19875  , -0.051368 , -0.023402 ,  0.08744  ,\n",
       "       -2.4938   ,  0.15427  ,  0.12373  , -0.0086429, -0.17007  ,\n",
       "       -0.519    , -0.29962  ,  0.24369  , -0.20535  , -0.24942  ,\n",
       "       -0.079362 ,  0.40986  , -0.10753  ,  0.098907 , -0.063449 ,\n",
       "        0.05373  ,  0.26206  ,  0.13207  , -0.067694 , -0.56168  ,\n",
       "       -0.18867  ,  0.14453  , -0.22469  , -0.28404  ,  0.20909  ,\n",
       "       -0.46989  ,  0.30992  , -0.13283  ,  0.041392 ,  0.11146  ,\n",
       "        0.17015  , -0.059407 , -0.16098  , -0.2211   , -0.0035877,\n",
       "       -0.22357  , -0.01852  , -0.23026  , -0.18824  ,  0.32997  ,\n",
       "        0.16287  , -0.52067  ,  0.17308  , -0.024264 , -0.041321 ,\n",
       "       -0.3241   , -0.44122  , -0.11114  ,  0.22684  , -0.10883  ,\n",
       "       -0.1278   , -0.16696  ,  0.051048 , -0.12131  ,  0.18038  ,\n",
       "        0.19793  ,  0.134    , -0.37113  ,  0.36008  ,  0.092685 ,\n",
       "       -0.30263  ,  0.16565  , -0.10863  , -0.29565  ,  0.26143  ,\n",
       "        0.13369  , -0.090181 ,  0.021989 , -0.093353 , -0.20325  ,\n",
       "       -0.2008   ,  0.20721  ,  0.17208  , -0.20199  ,  0.043315 ,\n",
       "        0.17768  ,  0.57448  , -0.45917  , -0.077197 ,  0.12051  ,\n",
       "        0.07209  , -0.095313 ,  0.10973  ,  0.22375  ,  0.045804 ,\n",
       "       -0.13573  ,  0.14041  , -0.11364  , -0.46605  , -0.43262  ,\n",
       "       -0.058678 ,  0.19043  , -0.40867  ,  0.30509  ,  0.18542  ,\n",
       "        0.095309 , -0.42329  , -0.15225  , -0.13827  ,  0.18119  ,\n",
       "        0.14755  , -0.053628 ,  0.031298 ,  0.65695  , -0.1717   ,\n",
       "        0.23649  , -0.34742  , -0.17438  , -0.085304 ,  0.37687  ,\n",
       "        0.21322  , -0.13184  , -0.35197  , -0.14072  ,  0.2332   ,\n",
       "        0.21014  , -0.14763  ,  0.047515 , -0.27979  ,  0.090331 ,\n",
       "       -0.15565  ,  0.42803  , -0.019297 ,  0.012198 ,  0.036031 ,\n",
       "       -0.10396  ,  0.11014  ,  0.13458  ,  0.2775   ,  0.36225  ,\n",
       "       -0.35591  , -0.16877  , -0.41201  ,  0.070133 , -0.27769  ,\n",
       "        0.13739  , -0.057831 ,  0.19277  ,  0.11131  ,  0.53696  ,\n",
       "        0.0093424, -0.26107  , -0.38663  ,  0.040653 ,  0.18617  ,\n",
       "        0.26312  ,  0.12212  , -0.030012 ,  0.096286 ,  0.47376  ,\n",
       "       -0.21633  ,  0.10798  , -0.17703  ,  0.22116  ,  0.6726   ,\n",
       "        0.065036 , -0.017414 , -0.048585 , -0.090863 ,  0.28591  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"happy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319911, 300)\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix for all unique toxens in preprocessed text\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "## EMBEDDINGS_DIMENSION =  ## dimension of glove vectors\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "num_words_in_embedding = 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "print(embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Define and train a LSTM Neural Net for classifying toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "def model2_fn(embedding_matrix,train_data,train_labels,cv_data,cv_labels):\n",
    "    \"\"\" Model Architecture\"\"\"\n",
    "    BATCH_SIZE = 512\n",
    "    LSTM_UNITS = 128\n",
    "    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "    NUM_EPOCHS = 4\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                EMBEDDINGS_DIMENSION,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    \n",
    "    result = Dense(128, activation='sigmoid')(hidden)\n",
    "    result = Dense(2, activation='sigmoid')(result)\n",
    "    \n",
    "   \n",
    "    # Compile model.\n",
    "    print('compiling model')\n",
    "    input_layer, output_layer =sequence_input, result\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "    # Train model.\n",
    "    print('training model')\n",
    "    model.fit(train_data,\n",
    "          train_labels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=(cv_data, cv_labels),\n",
    "          verbose=2)\n",
    "    model.save('../input/mymodel/my_model.h5')\n",
    "    return(model)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n",
      "training model\n",
      "Train on 1209265 samples, validate on 595609 samples\n",
      "Epoch 1/4\n",
      " - 797s - loss: 0.1441 - acc: 0.9447 - val_loss: 0.1303 - val_acc: 0.9477\n",
      "Epoch 2/4\n",
      " - 789s - loss: 0.1281 - acc: 0.9493 - val_loss: 0.1272 - val_acc: 0.9491\n",
      "Epoch 3/4\n",
      " - 764s - loss: 0.1232 - acc: 0.9509 - val_loss: 0.1228 - val_acc: 0.9508\n",
      "Epoch 4/4\n",
      " - 777s - loss: 0.1194 - acc: 0.9520 - val_loss: 0.1222 - val_acc: 0.9511\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "import keras\n",
    "if os.path.exists('../input/mymodel/my_model.h5'):\n",
    "    print(\"Loading model\")\n",
    "    model2 = keras.models.load_model('../input/mymodel/my_model.h5')\n",
    "else:\n",
    "    model2=model2_fn(embedding_matrix,train_data,train_labels,cv_data,cv_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate model predictions on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict validation data\n",
    "MODEL_NAME='model2'\n",
    "cv_pred= model2.predict(cv_data) # predict the tokenize cv data\n",
    "\n",
    "cv_model_data=X_cv\n",
    "cv_model_data[MODEL_NAME]=cv_pred[:, 1] # combine the X_cv with the predicted y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>christian</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>preprocessed_data</th>\n",
       "      <th>model2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1331671</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>military members disobey orders risk also obey...</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755024</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>shall call glancing blow directed middle class...</td>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909536</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ribeiro top earner sedgwick insurance practice...</td>\n",
       "      <td>0.791725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035788</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>anothet brilliant factoid keep em coming</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432339</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>not think would ever considered buy vw learned...</td>\n",
       "      <td>0.100668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100812</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>uk made horrible decision based fact free leav...</td>\n",
       "      <td>0.016328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678263</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>forgot one</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027361</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>boy wow sure came spending numbers really fast...</td>\n",
       "      <td>0.004299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292640</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>follow precedent jesus suggest pope answer sin...</td>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693871</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>hate buzz kill human interest story perfect ex...</td>\n",
       "      <td>0.021808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875359</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>idiots</td>\n",
       "      <td>0.586570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740967</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>blame wealthy job creators country siphoning f...</td>\n",
       "      <td>0.247123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510486</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>would require abandoning existing fractional r...</td>\n",
       "      <td>0.000143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913791</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>good grief though democrats would not sacked e...</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598232</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>fired obviously not picked intelligence not be...</td>\n",
       "      <td>0.069115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616136</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>well even though not big fan trudeau think han...</td>\n",
       "      <td>0.000145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144709</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>would found interesting analysis total tax bur...</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869252</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>years time trump gone reasonable ex higher tax...</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151478</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ivo not get high tech fancy green marketing pl...</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target   male  female  transgender  other_gender  heterosexual  \\\n",
       "1331671   False  False   False        False         False         False   \n",
       "755024    False  False   False        False         False         False   \n",
       "909536     True   True    True        False         False         False   \n",
       "1035788   False  False   False        False         False         False   \n",
       "1432339   False  False   False        False         False         False   \n",
       "100812    False  False   False        False         False         False   \n",
       "1678263   False  False   False        False         False         False   \n",
       "1027361   False  False   False        False         False         False   \n",
       "292640    False  False   False        False         False         False   \n",
       "693871    False  False   False        False         False         False   \n",
       "875359     True  False   False        False         False         False   \n",
       "1740967   False  False   False        False         False         False   \n",
       "1510486   False  False   False        False         False         False   \n",
       "913791    False  False   False        False         False         False   \n",
       "1598232   False  False   False        False         False         False   \n",
       "616136    False  False   False        False         False         False   \n",
       "1144709   False  False   False        False         False         False   \n",
       "869252    False  False   False        False         False         False   \n",
       "1151478   False  False   False        False         False         False   \n",
       "\n",
       "         homosexual_gay_or_lesbian  bisexual  other_sexual_orientation  \\\n",
       "1331671                      False     False                     False   \n",
       "755024                       False     False                     False   \n",
       "909536                       False     False                     False   \n",
       "1035788                      False     False                     False   \n",
       "1432339                      False     False                     False   \n",
       "100812                       False     False                     False   \n",
       "1678263                      False     False                     False   \n",
       "1027361                      False     False                     False   \n",
       "292640                       False     False                     False   \n",
       "693871                       False     False                     False   \n",
       "875359                       False     False                     False   \n",
       "1740967                      False     False                     False   \n",
       "1510486                      False     False                     False   \n",
       "913791                       False     False                     False   \n",
       "1598232                      False     False                     False   \n",
       "616136                       False     False                     False   \n",
       "1144709                      False     False                     False   \n",
       "869252                       False     False                     False   \n",
       "1151478                      False     False                     False   \n",
       "\n",
       "         christian  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
       "1331671      False  ...  False  False   False                    False   \n",
       "755024       False  ...  False  False   False                    False   \n",
       "909536       False  ...  False  False   False                    False   \n",
       "1035788      False  ...  False  False   False                    False   \n",
       "1432339      False  ...  False  False   False                    False   \n",
       "100812       False  ...  False  False   False                    False   \n",
       "1678263      False  ...  False  False   False                    False   \n",
       "1027361      False  ...  False  False   False                    False   \n",
       "292640        True  ...  False  False   False                    False   \n",
       "693871       False  ...  False  False   False                    False   \n",
       "875359       False  ...  False  False   False                    False   \n",
       "1740967      False  ...  False  False   False                    False   \n",
       "1510486      False  ...  False  False   False                    False   \n",
       "913791       False  ...  False  False   False                    False   \n",
       "1598232      False  ...  False  False   False                    False   \n",
       "616136       False  ...  False  False   False                    False   \n",
       "1144709      False  ...  False  False   False                    False   \n",
       "869252       False  ...  False  False   False                    False   \n",
       "1151478      False  ...  False  False   False                    False   \n",
       "\n",
       "         physical_disability  intellectual_or_learning_disability  \\\n",
       "1331671                False                                False   \n",
       "755024                 False                                False   \n",
       "909536                 False                                False   \n",
       "1035788                False                                False   \n",
       "1432339                False                                False   \n",
       "100812                 False                                False   \n",
       "1678263                False                                False   \n",
       "1027361                False                                False   \n",
       "292640                 False                                False   \n",
       "693871                 False                                False   \n",
       "875359                 False                                False   \n",
       "1740967                False                                False   \n",
       "1510486                False                                False   \n",
       "913791                 False                                False   \n",
       "1598232                False                                False   \n",
       "616136                 False                                False   \n",
       "1144709                False                                False   \n",
       "869252                 False                                False   \n",
       "1151478                False                                False   \n",
       "\n",
       "         psychiatric_or_mental_illness  other_disability  \\\n",
       "1331671                          False             False   \n",
       "755024                           False             False   \n",
       "909536                           False             False   \n",
       "1035788                          False             False   \n",
       "1432339                          False             False   \n",
       "100812                           False             False   \n",
       "1678263                          False             False   \n",
       "1027361                          False             False   \n",
       "292640                           False             False   \n",
       "693871                           False             False   \n",
       "875359                           False             False   \n",
       "1740967                          False             False   \n",
       "1510486                          False             False   \n",
       "913791                           False             False   \n",
       "1598232                          False             False   \n",
       "616136                           False             False   \n",
       "1144709                          False             False   \n",
       "869252                           False             False   \n",
       "1151478                          False             False   \n",
       "\n",
       "                                         preprocessed_data    model2  \n",
       "1331671  military members disobey orders risk also obey...  0.000111  \n",
       "755024   shall call glancing blow directed middle class...  0.000736  \n",
       "909536   ribeiro top earner sedgwick insurance practice...  0.791725  \n",
       "1035788           anothet brilliant factoid keep em coming  0.000196  \n",
       "1432339  not think would ever considered buy vw learned...  0.100668  \n",
       "100812   uk made horrible decision based fact free leav...  0.016328  \n",
       "1678263                                         forgot one  0.000149  \n",
       "1027361  boy wow sure came spending numbers really fast...  0.004299  \n",
       "292640   follow precedent jesus suggest pope answer sin...  0.000169  \n",
       "693871   hate buzz kill human interest story perfect ex...  0.021808  \n",
       "875359                                              idiots  0.586570  \n",
       "1740967  blame wealthy job creators country siphoning f...  0.247123  \n",
       "1510486  would require abandoning existing fractional r...  0.000143  \n",
       "913791   good grief though democrats would not sacked e...  0.002256  \n",
       "1598232  fired obviously not picked intelligence not be...  0.069115  \n",
       "616136   well even though not big fan trudeau think han...  0.000145  \n",
       "1144709  would found interesting analysis total tax bur...  0.000107  \n",
       "869252   years time trump gone reasonable ex higher tax...  0.000249  \n",
       "1151478  ivo not get high tech fancy green marketing pl...  0.000724  \n",
       "\n",
       "[19 rows x 27 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model_data[1:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8. Define bias metrics, then evaluate our new model for bias using the validation set prediction\n",
    "Taken from benchmark kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Calculate the final score\n",
    "Code taken from benchmark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    \"\"\" Calculate overall AUC \"\"\"\n",
    "    \n",
    "    true_labels = df['target']\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "    \n",
    "    \n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    \"\"\" Compute the final metric\"\"\"\n",
    "    print(power_mean(bias_df[SUBGROUP_AUC], POWER))\n",
    "    print(power_mean(bias_df[BPSN_AUC], POWER))\n",
    "    print(power_mean(bias_df[BNSP_AUC], POWER))\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    print(OVERALL_MODEL_WEIGHT)\n",
    "    print(overall_auc)\n",
    "    print(bias_score)\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8682228932796209\n",
      "0.8792234594705047\n",
      "0.9579244672201932\n",
      "0.25\n",
      "0.9585494302688679\n",
      "0.9017902733234395\n",
      "final_metric:  0.9159800625597966\n"
     ]
    }
   ],
   "source": [
    "bias_metrics_df2 = compute_bias_metrics_for_model(cv_model_data, identity_attribute, MODEL_NAME, 'target')\n",
    "bias_metrics_df2_mod=bias_metrics_df2[0:22]\n",
    "final_metric=get_final_metric(bias_metrics_df2_mod, calculate_overall_auc(cv_model_data, MODEL_NAME))\n",
    "print(\"final_metric: \",final_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7097320</th>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097321</th>\n",
       "      <td>0.008370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097322</th>\n",
       "      <td>0.034457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097323</th>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097324</th>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction\n",
       "id                 \n",
       "7097320    0.000111\n",
       "7097321    0.008370\n",
       "7097322    0.034457\n",
       "7097323    0.000234\n",
       "7097324    0.000206"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../Data/sample_submission.csv', index_col='id')\n",
    "sequences_test= tokenizer.texts_to_sequences(test_data['comment_text'])\n",
    "\n",
    "test_df = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "submission['prediction'] = model2.predict(test_df)[:, 1]\n",
    "submission.to_csv('submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final_metric score is  0.9159800625597966"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
